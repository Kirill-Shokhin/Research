{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba or first Selective SSM\n",
    "\n",
    "![image (7)](https://github.com/Kirill-Shokhin/Research/assets/46619252/b6209055-8de7-43ae-902e-f116735ef9b2)\n",
    "\n",
    "*In the era of ubiquitous Transformer dominance, where they consumed more and more silicon chips, and it seemed that things couldn't get any better, with the cost of each new token skyrocketing exponentially from the previous ones, she emerged - **Mamba**.*\n",
    "\n",
    "Transformers have made a profound impact on the field of Deep Learning, showcasing remarkable efficiency. However, they face a significant limitation in terms of input sequence length (context) due to their quadratic computational complexity. Most models operate with a context length of less than 10 000, rendering them impractical for tasks with large volumes of input data. Despite various rumors swirling around, it would indeed be strange to witness a strong AI that could be chatted into oblivion within a matter of minutes.\n",
    "\n",
    "|   | Complexity | Context length | Throughput |\n",
    "| :---:  | :---:  | :---:  | :---:  |\n",
    "| Transformer  | $L^2$   | $\\sim 10^3$  |  $x$ |\n",
    "| Mamba  | $L$  | $\\sim 10^6$  | $5x$ |\n",
    "\n",
    "Mamba is built on a fundamentally different approach - **state space model (SSM)**, which, although much older than Transformers, had not shown sufficient effectiveness in deep learning, particularly as a language model. Mamba boasts a linear computational dependency and five times higher throughput than Transformers. The authors tested their innovation on a series of models with parameters **up to 2.8 billion**, which may not yet rival **ChatGPT**, but already outperforms current top language models in its weight category. The context length was chosen to match that of the corresponding Transformer. So a context size of a million was only tested on simple synthetic tests. However, this is also significant, as neither Transformers nor convolutions were able to handle these tests. In this article, we will delve into the mathematics behind the new architecture, exploring its advantages and limitations in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear state space model\n",
    "### Continuous case\n",
    "\n",
    "The state space model, upon which the entire concept is built, appears in continuous form as follows:\n",
    "\n",
    "$$ \\boldsymbol{\\dot h}(t) = \\boldsymbol{Ah}(t)+\\boldsymbol{Bx}(t) $$\n",
    "\n",
    "$$\\boldsymbol{y}(t) = \\boldsymbol{Ch}(t)+\\boldsymbol{Dx}(t) $$\n",
    "\n",
    "Using such a model, a differential equation of the `$N$`-th order can be expressed as `$N$` first-order equations in matrix form, where `$\\boldsymbol{h}(t)$` is the state vector containing derivatives from order `$0$` to `$N-1$`, `$x(t)$` - input signal, and `$y(t)$`  - output signal. Thus, the complexity of the described system nonlinearly increases with `$N$`.\n",
    "\n",
    "Alternatively, one can view the model as follows: a one-dimensional signal `$x(t)$` is mapped to an `$N$`-dimensional latent state `$\\boldsymbol{h}(t)$`, which is then projected onto the output signal `$y(t)$`.\n",
    "\n",
    "To interact with the model in vector form of finite dimensionality, it needs to be discretized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization\n",
    "The second equation is discretized straightforwardly: $\\boldsymbol{y_k} = \\boldsymbol{y}(k\\Delta) \\rightarrow \\boldsymbol{y_k} = \\boldsymbol{Ch_k}+\\boldsymbol{Dx_k}$. Let's delve into the details for the first equation:\n",
    "\n",
    "#### General solution\n",
    "Let's multiply the first equation by `$e^{-\\boldsymbol{A}t}$`:\n",
    "\n",
    "$$ e^{-\\boldsymbol{A}t} \\boldsymbol{\\dot h}(t) - e^{-\\boldsymbol{A}t} \\boldsymbol{Ah}(t) = e^{-\\boldsymbol{A}t} \\boldsymbol{Bx}(t)$$\n",
    "\n",
    "$$ \\frac{d}{dt} (e^{-\\boldsymbol{A}t} \\boldsymbol{h}(t)) = e^{-\\boldsymbol{A}t} \\boldsymbol{Bx}(t)$$\n",
    "\n",
    "Then the general solution for the continuous model will be:\n",
    "\n",
    "$$\\boldsymbol{h}(t) = e^{\\boldsymbol{A}t} \\boldsymbol{h}(0) + \\int_0^t{e^{\\boldsymbol{A}(t-\\tau)}} \\boldsymbol{Bx}(\\tau) d\\tau$$\n",
    "\n",
    "Discretization step `$\\Delta \\Rightarrow \\boldsymbol{x_k} = \\boldsymbol{x}(k \\Delta), \\boldsymbol{h_k} = \\boldsymbol{h}(k \\Delta)$`:\n",
    "\n",
    "$$ \\boldsymbol{h_k} = e^{\\boldsymbol{A}k\\Delta} \\boldsymbol{h}(0) + \\int_0^{k\\Delta}{e^{\\boldsymbol{A}(k\\Delta-\\tau)}} \\boldsymbol{Bx}(\\tau) d\\tau $$\n",
    "\n",
    "... (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... (continuation of the previous cell)\n",
    "\n",
    "$$ \\boldsymbol{h_{k+1}} = e^{\\boldsymbol{A}\\Delta} \\left[ e^{\\boldsymbol{A}k\\Delta} \\boldsymbol{h}(0) + \\int_0^{k\\Delta}{e^{\\boldsymbol{A}(k\\Delta-\\tau)}} \\boldsymbol{Bx}(\\tau) d\\tau\\right] +\\int_{k\\Delta}^{(k+1)\\Delta} e^{\\boldsymbol{A}\\left[(k+1)\\Delta-\\tau\\right]} \\boldsymbol{Bx}(\\tau) d\\tau = $$\n",
    "\n",
    "... (continued)"
   ]
  },
  ... (additional cells for the rest of the article)
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
